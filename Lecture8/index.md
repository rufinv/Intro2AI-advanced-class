## Lecture 8 - Attention/Transformers in NLP
Teacher: Romain Bielawski (ANITI)


<div style="color: red"><span style="font-weight: bold">Very important:</span> due to a strong response in the community, we are splitting the class in 2 sessions. This page is for the <span style="font-weight: bold">advanced</span> session, appropriate for students with prior training in Maths, Engineering and/or Computer Science. If you are here by mistake, go to <a href="https://rufinv.github.io/Intro2AI-class/">[the main/basic session page]</a>. </div>


### Contents

* Vanishing gradient
* Attention in LSTM
* Self attention and transformer
* Embedding : Bert
* Generation : GPT
* Application : Story generation


### Prerequisites:
Knowledge about neural networks principles; knowledge of several NN layer types; gradient descent & backpropagation; basics of NLP; RNN principles


### Further reading:

---
#### [(Back to Main Page)](../index.md)
